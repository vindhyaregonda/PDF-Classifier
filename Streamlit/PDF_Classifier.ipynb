{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-TEoQkY2gxrU",
        "outputId": "cd25c1f5-b674-4300-b2b2-804ba7fe123b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Collecting PyMuPDFb==1.24.9 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.9 PyMuPDFb-1.24.9\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 sentence-transformers-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install pytesseract\n",
        "!pip install transformers\n",
        "!pip install PyMuPDF pillow\n",
        "!pip install sentence-transformers\n",
        "!pip install streamlit\n",
        "!npm install -g localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit"
      ],
      "metadata": {
        "id": "4HwIupVWn2cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from bs4 import BeautifulSoup\n",
        "from io import BytesIO\n",
        "import PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import requests\n",
        "import fitz  # PyMuPDF\n",
        "import random\n",
        "import time\n",
        "import io\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, precision_score\n",
        "import pickle\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "st.title(\"PDF Classifier\")\n",
        "model_choice = st.selectbox('Select Model', ['Machine Learning', 'Deep Learning'])\n",
        "\n",
        "class DataPreprocess:\n",
        "    def __init__(self, filepath):\n",
        "        self.df = pd.read_csv(filepath)\n",
        "\n",
        "    def display(self):\n",
        "        print(self.df.head())\n",
        "        print()\n",
        "        print(self.df.info())\n",
        "\n",
        "    # This function gives processed dataframe with correct urls\n",
        "    def pre_processed(self):\n",
        "        no_dup = self.df.drop_duplicates()\n",
        "        corrected_df = no_dup[no_dup['datasheet_link'] != '-']\n",
        "        corrected_df = corrected_df.dropna()\n",
        "\n",
        "        # corrected_df = corrected_df[~corrected_df.apply(lambda row: row.astype(str).str.contains('8e9daddd-82b0-4ed4-a656-a8aa011ea6d3').any(), axis=1)]\n",
        "        corrected_df = corrected_df[corrected_df['datasheet_link'].str.startswith('http')]\n",
        "\n",
        "        # corrected_df = corrected_df[corrected_df['datasheet_link'].str.endswith('.pdf')]\n",
        "        corrected_df = corrected_df.reset_index(drop=True)\n",
        "\n",
        "        return corrected_df\n",
        "\n",
        "    def incorrect_url(self):\n",
        "        incorrect_url_df = self.df[~self.df['datasheet_link'].str.startswith('http')]\n",
        "        incorrect_url_df['datasheet_link'] = 'https:' + incorrect_url_df['datasheet_link']\n",
        "        return incorrect_url_df\n",
        "\n",
        "    def processed(self):\n",
        "        df1 = self.pre_processed()\n",
        "        df2 = self.incorrect_url()\n",
        "        return pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "\n",
        "class ExtractData:\n",
        "    def __init__(self, image_dir='images', max_retries=5):\n",
        "        self.image_dir = image_dir\n",
        "        self.max_retries = max_retries\n",
        "        if not os.path.exists(image_dir):\n",
        "            os.makedirs(image_dir)\n",
        "\n",
        "    def url_process(self, url): # Extracts text from the PDF\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "                response = requests.get(url, headers=headers, timeout=20)\n",
        "                time.sleep(1)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    pdf_file = io.BytesIO(response.content)\n",
        "                    doc = fitz.open(stream=pdf_file, filetype='pdf')\n",
        "                    text = \"\"\n",
        "\n",
        "                    # Extract text from the first page\n",
        "                    for page_num in range(min(1, len(doc))):\n",
        "                        page = doc.load_page(page_num)\n",
        "                        page_text = page.get_text()\n",
        "                        text += page_text\n",
        "\n",
        "                    if text.strip():\n",
        "                        return text, []\n",
        "\n",
        "                    # Extract images if text extraction fails\n",
        "                    image_paths = self.extract_images(doc)\n",
        "                    return text, image_paths\n",
        "            except Exception as e:\n",
        "                # print(url)\n",
        "                print(f\"Error extracting text from PDF: {e}\")\n",
        "\n",
        "            sleep_time = 2 * attempt + random.uniform(0, 1)\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "        return None, []\n",
        "\n",
        "    def image_process(self, image_path): # Extracts text from image\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path)\n",
        "            text = pytesseract.image_to_string(image)\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            # print(f\"Error extracting text from image: {e}\")\n",
        "            return None\n",
        "\n",
        "    def extract_images(self, doc):  # Extract images and returns image paths\n",
        "\n",
        "        image_paths = []\n",
        "        for page_num in range(min(1, len(doc))):\n",
        "            page = doc.load_page(page_num)\n",
        "            image_list = page.get_images(full=True)\n",
        "            for img_index, img in enumerate(image_list):\n",
        "                xref = img[0]\n",
        "                base_image = doc.extract_image(xref)\n",
        "                image_bytes = base_image[\"image\"]\n",
        "                image_ext = base_image[\"ext\"]\n",
        "                image_filename = f\"{self.image_dir}/page_{page_num}_img_{img_index}.{image_ext}\"\n",
        "                with open(image_filename, \"wb\") as img_file:\n",
        "                    img_file.write(image_bytes)\n",
        "                image_paths.append(image_filename)\n",
        "        return image_paths\n",
        "\n",
        "    def process_row(self, row):  # process each url from dataframe to extract text.\n",
        "        url = row['datasheet_link']\n",
        "        text, image_paths = self.url_process(url)\n",
        "        if not text:  # If no text was extracted from the PDF, try OCR on images\n",
        "            text = \"\"\n",
        "            for img_path in image_paths:\n",
        "                img_text = self.image_process(img_path)\n",
        "                if img_text:\n",
        "                    text += \" \" + img_text\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_text(self, df): # uses multiprocessing to extract text from url\n",
        "\n",
        "        df['content'] = \"\"   # Text will be stored in new column\n",
        "\n",
        "        with ProcessPoolExecutor() as executor:\n",
        "            # Create futures\n",
        "            futures = {executor.submit(self.process_row, row): idx for idx, row in df.iterrows()}\n",
        "            for future in tqdm(as_completed(futures), total=len(futures), desc='Processing rows'):\n",
        "                idx = futures[future]\n",
        "                try:\n",
        "                    df.at[idx, 'content'] = future.result()\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing row {idx}: {e}\")\n",
        "                    df.at[idx, 'content'] = \"\"\n",
        "\n",
        "        return df\n",
        "\n",
        "# DL model\n",
        "class PDFTextDataset(Dataset):\n",
        "    def __init__(self, filepath):\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.df = pd.read_csv(filepath)\n",
        "\n",
        "        self.df['content'] = self.df['content'].fillna('')\n",
        "        self.df['embedding'] = self.df['content'].apply(lambda x: self.model.encode(x))\n",
        "\n",
        "        embeddings = self.df['embedding'].tolist()\n",
        "        embedding_size = len(embeddings[0]) if embeddings else 0\n",
        "        self.X = torch.tensor(embeddings, dtype=torch.float32)\n",
        "\n",
        "        self.test_data = TensorDataset(self.X)\n",
        "\n",
        "        save_path = \"/content/drive/MyDrive/Parspec/test_embedding_streamlit.pkl\"\n",
        "        with open(save_path, 'wb') as f:\n",
        "            pickle.dump(embeddings, f)\n",
        "\n",
        "    def get_test_dataloader(self, batch_size=32):\n",
        "        test_loader = DataLoader(self.test_data, batch_size=batch_size, shuffle=False)\n",
        "        return test_loader\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=384, hidden_dim=128, output_dim=4, dropout_rate=0.2):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        # self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        # x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        # x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.load_state_dict(torch.load(\"/content/drive/MyDrive/Parspec/model_data.pth\"))\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "\n",
        "    label_mapping = {2: \"lighting\", 1: \"fuses\", 0: \"cable\", 3: \"others\"}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs = batch[0]  # Extract features\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.numpy())\n",
        "\n",
        "            for pred in predicted.numpy():\n",
        "                print(f\"Prediction: {pred}\")\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_preds_mapped = [label_mapping[pred] for pred in all_preds]\n",
        "    return all_preds_mapped\n",
        "\n",
        "\n",
        "# ML model\n",
        "def MLdataset(filepath):\n",
        "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        df = pd.read_csv(filepath)\n",
        "\n",
        "        df['content'] = df['content'].fillna('')\n",
        "        df['embedding'] = df['content'].apply(lambda x: model.encode(x))\n",
        "\n",
        "        embeddings = df['embedding'].tolist()\n",
        "\n",
        "        X_test_np = np.array(embeddings)\n",
        "        return X_test_np\n",
        "\n",
        "def evaluate_ML(X):\n",
        "    xgb_model = xgb.XGBClassifier()\n",
        "    xgb_model.load_model('/content/drive/MyDrive/Parspec/train_data/xgb_model.json')\n",
        "    y_pred = xgb_model.predict(X)\n",
        "\n",
        "    label_mapping = {2: \"lighting\", 1: \"fuses\", 0: \"cable\", 3: \"others\"}\n",
        "    y_pred_mapped = [label_mapping[pred] for pred in y_pred]\n",
        "    return y_pred_mapped\n",
        "\n",
        "def classifypdf(csv_file):\n",
        "\n",
        "    csv_path = \"uploaded_file.csv\"\n",
        "    with open(csv_path, \"wb\") as f:\n",
        "      f.write(csv_file.getbuffer())\n",
        "\n",
        "    data_processor = DataPreprocess(csv_path)\n",
        "    preprocessed_data = data_processor.processed()\n",
        "\n",
        "    extractor = ExtractData()\n",
        "    df = extractor.extract_text(preprocessed_data)\n",
        "    df.to_csv('final_df.csv')\n",
        "\n",
        "    if model_choice == 'Machine Learning':\n",
        "      dataset = MLdataset('final_df.csv')\n",
        "      results = evaluate_ML(dataset)\n",
        "      return results\n",
        "\n",
        "    elif model_choice == 'Deep Learning':\n",
        "      dataset = PDFTextDataset('final_df.csv')\n",
        "      test_loader = dataset.get_test_dataloader()\n",
        "      model2 = TextClassifier()\n",
        "      results = evaluate_model(model2, test_loader)\n",
        "      return results\n",
        "\n",
        "\n",
        "def main():\n",
        "  uploaded_file = st.file_uploader(\"Upload a CSV file containing PDF URLS\", type = 'csv')\n",
        "  if uploaded_file is not None:\n",
        "      st.write(\"Processing the file\")\n",
        "      results = classifypdf(uploaded_file)\n",
        "      st.write(\"Classification results: \")\n",
        "      st.write(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UGRAN42jh95",
        "outputId": "cf0dc072-43e7-4c61-fbbe-38e1c76d0d25"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch your IP address. Save the IP address running this cell\n",
        "\n",
        "!curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "oTltykXjmmPD"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After execution, this will produce a link. In Tunnel Password, enter the above saved IP address\n",
        "\n",
        "!streamlit run app.py &>./logs.txt & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xjh0lBo7mrh1",
        "outputId": "23d58224-7da8-4efb-8ef8-0022f6925e57"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your url is: https://honest-rice-pay.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}